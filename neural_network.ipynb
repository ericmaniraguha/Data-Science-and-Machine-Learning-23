{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition: \n",
    "* A piece of software \n",
    "* A model of the brain \n",
    "* Capable of reproducing some behaviors of the brain \n",
    "* Capable of learning and classifying\n",
    "\n",
    "What a neural network is not?\n",
    "\n",
    "* A series of if ... then statements.\n",
    "* All there is to machine learning \n",
    "* The only type of classifier\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neural network is a type of machine learning model that is inspired by the structure and function of the human brain. It consists of layers of interconnected nodes, or \"neurons,\" which can process and transmit information. The nodes in a neural network are usually organized into layers, with the input layer receiving the initial inputs, one or more hidden layers processing the inputs, and an output layer producing the final outputs.\n",
    "\n",
    "The connections between the nodes can have a weight associated with them, which can be adjusted during training to minimize the error between the network's predictions and the true values. This process is called \"training\" or \"learning,\" and it is typically done using a variant of gradient descent algorithm.\n",
    "\n",
    "Neural networks have been used to solve a wide range of problems in machine learning, including image recognition, speech recognition, natural language processing, and control systems. There are many types of neural network architectures, including feedforward neural networks, recurrent neural networks, and convolutional neural networks, each of which is well-suited to different types of problems."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Types of Machine learning algorithms\n",
    "1. Supervised learning \n",
    "* Regressive (Linear, logistic, exponential)\n",
    "* Classification (Support vector machines and neural networks)\n",
    "2. Unsupervised learning\n",
    "* Clustering (medical imganing and recommeder systems)\n",
    "* Anomaly detection (to find anomaly condition and outliers (detecting credit card frauds, typos text or medical condition))\n",
    "* Neural networks (Self organizing maps)\n",
    "3. Reinforcement Learning\n",
    "* video game playing AI\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "There are several types of neural network architectures, each of which is suited to different types of problems. Some common types of neural networks include:\n",
    "\n",
    "* Feedforward Neural Networks: Also known as multi-layer perceptrons (MLPs), these networks have a simple structure in which the input is passed through a series of layers without loops. The output from one layer serves as the input for the next layer.\n",
    "\n",
    "* Recurrent Neural Networks (RNNs): These networks have loops, which allow them to maintain a hidden state that can depend on previous inputs. This makes them well-suited for processing sequential data, such as speech or time series.\n",
    "\n",
    "* Convolutional Neural Networks (CNNs): These networks are inspired by the structure of the visual cortex and are commonly used in image and video processing. They are made up of multiple layers, including convolutional layers, which are used to learn local patterns in the data, and pooling layers, which are used to reduce the dimensionality of the data.\n",
    "\n",
    "* Long Short-Term Memory (LSTM): LSTMs are a type of RNN that have the ability to learn long-term dependencies. LSTM are designed to overcome the problem of vanishing gradients.\n",
    "\n",
    "* Gated Recurrent Unit (GRU): GRUs are another type of RNN that have been proposed as a simpler alternative to LSTMs. They also have the ability to learn long-term dependencies.\n",
    "\n",
    "* Autoencoder: They are unsupervised neural networks which are trained to reconstruct the input. The idea is to learn a compact representation of the input by training the network to reconstruct the input from a lower-dimensional encoding.\n",
    "\n",
    "These are some of the most common types of neural networks, but new architectures are being developed all the time, each with their own strengths and weaknesses. Choosing the right architecture for a given problem is an important aspect of building a successful neural network model.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AND Gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gate:\n",
      "0 0 = 0.0000003059\n",
      "0 1 = 0.0066928509\n",
      "1 0 = 0.0066928509\n",
      "1 1 = 0.9933071491\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    \"\"\"A single neuron with the sigmoid activation function.\n",
    "       Attributes:\n",
    "          inputs: The number of inputs in the perceptron, not counting the bias.\n",
    "          bias:   The bias term. By default it's 1.0.\"\"\"\n",
    "\n",
    "    def __init__(self, inputs, bias = 1.0):\n",
    "        \"\"\"Return a new Perceptron object with the specified number of inputs (+1 for the bias).\"\"\" \n",
    "        self.weights = (np.random.rand(inputs+1) * 2) - 1 \n",
    "        self.bias = bias\n",
    "\n",
    "    def run(self, x):\n",
    "        \"\"\"Run the perceptron. x is a python list with the input values.\"\"\"\n",
    "        x_sum = np.dot(np.append(x,self.bias),self.weights)\n",
    "        return self.sigmoid(x_sum)\n",
    "\n",
    "    def set_weights(self, w_init):\n",
    "        \"\"\"Set the weights. w_init is a python list with the weights.\"\"\"\n",
    "        self.weights = np.array(w_init)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"Evaluate the sigmoid function for the floating point input x.\"\"\"\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "#test code\n",
    "neuron = Perceptron(inputs=2)\n",
    "neuron.set_weights([10,10,-15]) #AND\n",
    "\n",
    "print(\"Gate:\")\n",
    "print (\"0 0 = {0:.10f}\".format(neuron.run([0,0])))\n",
    "print (\"0 1 = {0:.10f}\".format(neuron.run([0,1])))\n",
    "print (\"1 0 = {0:.10f}\".format(neuron.run([1,0])))\n",
    "print (\"1 1 = {0:.10f}\".format(neuron.run([1,1])))\n",
    "# Results will show that our neural network can work as an and gate\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OR Gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gate:\n",
      "0 0 = 0.0000453979\n",
      "0 1 = 0.9933071491\n",
      "1 0 = 0.9933071491\n",
      "1 1 = 0.9999999979\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    \"\"\"A single neuron with the sigmoid activation function.\n",
    "       Attributes:\n",
    "          inputs: The number of inputs in the perceptron, not counting the bias.\n",
    "          bias:   The bias term. By default it's 1.0.\"\"\"\n",
    "\n",
    "    def __init__(self, inputs, bias = 1.0):\n",
    "        \"\"\"Return a new Perceptron object with the specified number of inputs (+1 for the bias).\"\"\" \n",
    "        self.weights = (np.random.rand(inputs+1) * 2) - 1 \n",
    "        self.bias = bias\n",
    "\n",
    "    def run(self, x):\n",
    "        \"\"\"Run the perceptron. x is a python list with the input values.\"\"\"\n",
    "        x_sum = np.dot(np.append(x,self.bias),self.weights)\n",
    "        return self.sigmoid(x_sum)\n",
    "\n",
    "    def set_weights(self, w_init):\n",
    "        \"\"\"Set the weights. w_init is a python list with the weights.\"\"\"\n",
    "        self.weights = np.array(w_init)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"Evaluate the sigmoid function for the floating point input x.\"\"\"\n",
    "        return 1/(1+np.exp(-x))\n",
    "\n",
    "\n",
    "#test code\n",
    "neuron = Perceptron(inputs=2)\n",
    "neuron.set_weights([15,15,-10]) #OR\n",
    "\n",
    "# Challenge: Write your OR weights\n",
    "\n",
    "print(\"Gate:\")\n",
    "print (\"0 0 = {0:.10f}\".format(neuron.run([0,0])))\n",
    "print (\"0 1 = {0:.10f}\".format(neuron.run([0,1])))\n",
    "print (\"1 0 = {0:.10f}\".format(neuron.run([1,0])))\n",
    "print (\"1 1 = {0:.10f}\".format(neuron.run([1,1])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1f6a27bcfbe46a917dbd192f4a82657396dda26148bae633192e8d28c70725f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
